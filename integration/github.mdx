---
title: "GitHub"
---

This method allows you to pull custom code to load the model, and write custom pre-processing and post-processing functions. This method is best suited if you want to run a pipeline of models. 

Language Supported: **Python** 


### Connect the GitHub Account

1. Go to the [Inferless Dashboard](https://app.inferless.com) and click on the **Integrations** tab.
2. Select the Github option and click on the **Connect** button.
3. You will be redirected to the Github login page.
4. Enter your Github credentials and click on the **Authorize Inferless** button. ( You can control the Repos you want inferless to access )
5. You will be redirected back to the Inferless dashboard.


### Preparing the repository 

We have created a Template repository that you can use as a base to inject your code you can find a sample here with the GPT Neo model. 

Github Repo: [https://github.com/infer-less/template-method](https://github.com/infer-less/template-method)

```python
## Implement the Load function here for the model 
    def initialize(self):
        self.generator = pipeline("text-generation", model="EleutherAI/gpt-neo-125M",device=0)

    
# Function to perform inference 
    def infer(self, inputs):
        # inputs is a dictionary where the keys are input names and values are actual input data
        # e.g. in the below code the input name is "prompt"
        prompt = inputs["prompt"]
        pipeline_output = self.generator(prompt, do_sample=True, min_length=20)
        generated_txt = pipeline_output[0]["generated_text"]
        # The output generated by the infer function should be a dictionary where keys are output names and values are actual output data
        # e.g. in the below code the output name is "generated_txt"
        return {"generated_text": generated_txt}

# perform any cleanup activity here
    def finalize(self):
        self.pipe = None
```

You also need to create a schema file to define the input parameters. 

input\_schema.py 

```input_schema
INPUT_SCHEMA = {
    "prompt": {
        'datatype': 'STRING',
        'required': True,
        'shape': [1],
        'example': ["There is a fine house in the forest"]
    }
}
```


### Importing the model via Dashbaord 


1. Go to the [Inferless Dashboard](https://app.inferless.com) and click on the **Models** tab.
2. Click on the **Add Model** button.
3. Select the **GitHub** option and select the github account 
4. Enter the **Repository URL**.
5. Select the **Branch** name.
6. Select the machine configuration for deployment. 
7. Optionally you can configure custom runtime, by default will use inferless default runtime.
8. Optionally you can also mount volumes for the model.


#### Video Walkthourgh 

<iframe width="560" height="315" src="https://www.youtube.com/embed/dQw4w9WgXcQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


### Importing the model via CLI 


Go to directory where you have cloned the repository and run the following command 

```bash

1. Install the Inferless CLI 

```bash
pip install --upgrade inferless
```

2. Run the following command to import the model 

```bash
inferless init 
```
You will be propted for the model information 


3. Run the following command to deploy the model 

```bash
inferless deploy 
```


#### Video Walkthourgh 

<iframe width="560" height="315" src="https://www.youtube.com/embed/dQw4w9WgXcQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

